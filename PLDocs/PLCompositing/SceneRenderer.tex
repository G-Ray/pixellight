\chapter{Scene renderer}


\paragraph{PixelLight Scene Renderer System}$\;$ \\
The scene renderer system of PixelLight consists of several render passes. One can add or remove scene renderer passes as wanted. The scene renderer passes can be categorized in the following way:
\begin{itemize}
\item{Fixed function: For legacy hardware without shader support and just fixed build in graphics features}
\item{Forward: A classic forward renderer using shaders. Each object is drawn per light again}
\item{Deferred: A modern deferred renderer approach performing for example lighting in image space}
\end{itemize}
This document only describes the deferred scene renderer of PixelLight.




\section{Deferred scene renderer}
The deferred scene renderer of PixelLight is, like other PixelLight scene renderer implementations, a collection of render passes working together to form the render pipeline. This scene renderer is using \emph{Image Based Lighting} (IBL) and requires Shader 3.0 as a minimum. The XML-file \emph{DeferredRendering.sr} describes the render passes to use and their order.
\begin{itemize}
\item{PLScene::SRPBegin (independent from the deferred scene renderer)}
\item{PLCompositing::SRPDeferredGBuffer (with gamma correction support)}
\item{PLCompositing::SRPDeferredHBAO}
\item{PLCompositing::SRPDeferredHDAO}
\item{PLCompositing::SRPDeferredAmbient}
\item{PLCompositing::SRPDeferredGlow}
\item{PLCompositing::SRPDeferredLighting (with gamma correction support)}
\item{PLCompositing::SRPDeferredGodRays}
\item{PLCompositing::SRPDeferredDepthFog}
\item{PLCompositing::SRPDeferredEdgeAA}
\item{PLCompositing::SRPDeferredDOF}
\item{PLCompositing::SRPEndHDR (independent from the deferred scene renderer)}
\item{PLCompositing::SRPDeferredGBufferDebug}
\end{itemize}


\paragraph{Terminology}$\;$ \\
Often, there are many names describing the same thing. Therefore here's a list of names PixelLight is using. (the first one)
\begin{itemize}
\item{\emph{view space} = \emph{camera space} = \emph{eye space}}
\item{\emph{clip space} = \emph{projection space}}
\end{itemize}


\paragraph{Data Driven}$\;$ \\
The deferred scene renderer is using a \emph{data driven} approach. This means that instead forcing the user to define shaders by hand, just material descriptions are provided by the user and the scene renderer has to interpret them as best as possible. This way, the user only has to create the data and materials once, and then it's possible to use one and the same data set on totally different scene renderer techniques without the need to rewrite all material descriptions and shaders. By using this concept, the scene renderer is also able to optimize automatically and to scale with the available hardware.


\paragraph{Über Shaders}$\;$ \\
The deferred scene renderer is using so called \emph{Über Shaders}. This simply means that shaders of the scene renderer are written within a high level shader language and making heavy usage of precompiler directives like \emph{\#ifdef}. Therefore, most times, there's only one high level shader that takes everything into account that is supported. During runtime, only the features a current material is really using is taken into account. This means that many versions of one shader are compiled during runtime resulting in effective shaders. This \emph{Über Shaders} may look confusing on the first look, but they have the big advantage of code reuse. If there's a bug, this bug will probably influence all or most compiled shader versions and therefore the possibility that it can be found and fixed is high. The bug only has to be fixed at one place, and not hundreds of places which would just lead to a high probability of even more bugs.




\subsection{GBuffer Layout}
Table ~\ref{Table:GBufferLayout}.
\begin{table}[htb]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		RT & R & G & B & A\\
		\hline
		\hline
		DS & Depth Buffer & Depth Buffer & Depth Buffer & Stencil\\
		\hline
		\hline
		RT0 & Albedo R & Albedo G & Albedo B & Ambient Occlusion\\
		\hline
		RT1 & Normal X & Normal Y & Depth & -\\
		\hline
		RT2 & Specular R & Specular G & Specular B & Specular Exponent\\
		\hline
		RT3 & Self illumination R & Self illumination G & Self illumination B & Glow Factor\\
		\hline
		\hline
	\end{tabular} 
	\caption{GBuffer Layout}
	\label{Table:GBufferLayout}
\end{table}
By default, the GBuffer is using FP16 render targets - but FP32 render targets can be enabled as well.


\paragraph{Stencil buffer}$\;$ \\
By default, the \emph{PLCompositing::SRPDeferredGBuffer} scene renderer pass writes into the stencil buffer whether or not a pixel has valid content. 1 within the stencil buffer means: The GBuffer has no information about the pixel because no geometry is covering it. Later on, for example within the \emph{PLCompositing::SRPDeferredAmbient} scene render pass, this stencil buffer information can be used to draw only pixels with valid GBuffer content, everything else is not drawn. This is quite useful if there's for example a bitmap or sky already drawn and should remain in the \emph{background}.


\paragraph{Parallax Mapping}$\;$ \\
Parallax Mapping as described in \url{http://www8.cs.umu.se/kurser/5DV051/VT09/lab/parallax_mapping.pdf}.


\paragraph{Two-Sided Polygons}$\;$ \\
Two-sided polygons are handled when filling the GBuffer by drawing the mesh a second time, but with flipped normal vectors. This is a simple, universal and robust way to solve this problem. As a result, there's no need to implement special two-sided lighting in later render passes.


\paragraph{Gamma correction}$\;$ \\
Usually, color textures like hand-painted images or photos are stored in sRGB space, therefore, they must be converted from sRGB to linear space during rendering. This is automatically done for the material parameters \emph{DiffuseMap}, \emph{LightMap}, \emph{EmissiveMap} and \emph{ReflectionMap}. If this wouldn't be done, the colors of this texture maps would look bleached out.
Cg source~code~\ref{Code::GBufferGammaCorrection} shows how the used gamma correction technique.
\begin{lstlisting}[float=htb,label=Code::GBufferGammaCorrection,caption={GBuffer gamma correction}]
float3 linearRGBColor = pow(sRGBColor, 2.2);
\end{lstlisting}




\subsubsection{GBuffer RT0}
The \textbf{rgb}-components of \emph{RT0} contain the albedo. The albedo is calculated using \emph{DiffuseMap.rgb * DiffuseColor.rgb}. The \textbf{a}-component of \emph{RT0} is used for ambient occlusion, either static\footnote{Some call it \emph{Baked Occ}} pre-calculated or dynamic\footnote{Later on within the scene renderer pipeline, for example \emph{PLCompositing::SRPDeferredHBAO} renders ambient occlusion into this alpha channel}.


\paragraph{Fresnel Reflection}$\;$ \\
Due to the fresnel effect, a surface becomes more reflective near grazing angle. Fresnel reflection is implemented as described within \url{http://developer.download.nvidia.com/SDK/9.5/Samples/DEMOS/Direct3D9/src/HLSL_FresnelReflection/docs/FresnelReflection.pdf}.
Fresnel reflection is controlled by using the \emph{IndexOfRefraction} and \emph{FresnelReflectionPower} material parameters.


\paragraph{Spherical Environment Mapping}$\;$ \\
If the given \emph{ReflectionMap} material parameter is a 2D map, spherical environment mapping as described within \url{http://www.ozone3d.net/tutorials/glsl_texturing_p04.php} is performed. The spherical map has to fulfil the following conditions:
\begin{itemize}
\item{The texture coordinate of the center of the map is (0,0), and the sphere's image has radius 1}
\item{The projection direction is along the z-axis}
\end{itemize}


\paragraph{Cubic Environment Mapping}$\;$ \\
If the given \emph{ReflectionMap} material parameter is a cube map, cubic environment mapping is performed. More information about cubic environment mapping can for example be found at \url{http://http.developer.nvidia.com/CgTutorial/cg_tutorial_chapter07.html}.




\subsubsection{GBuffer RT1}
The \textbf{rg}-components of \emph{RT1} contain the normal vector within view space. To save components within the GBuffer, only x and y of the normal vector are saved. The 3D normal vector is rebuild later by using z reconstruction. To archive this, the encode and decode functions from \url{http://aras-p.info/texts/CompactNormalStorage.html#method04spheremap} are used. Cg source code \ref{Code::NormalToGBuffer} shows how the xy-components of the normal vector are stored into the GBuffer.
\begin{lstlisting}[float=htb,label=Code::NormalToGBuffer,caption={Normal vector to GBuffer}]
// encodes a 3 component normal vector to a 2 component normal vector
float2 encodeNormalVector(float3 normal)
{
	float p = sqrt(normal.z*8 + 8);
	return float2(normal.xy/p + 0.5f);
}
\end{lstlisting}
Cg source code \ref{Code::GBufferToNormal} shows how the normal vector is restored from the GBuffer.
\begin{lstlisting}[float=htb,label=Code::GBufferToNormal,caption={GBuffer to normal vector}]
// decodes a 2 component normal vector to a 3 component normal vector
float3 decodeNormalVector(float2 normal)
{
    float2 fenc = normal*4 - 2;
    float f = dot(fenc, fenc);
    float g = sqrt(1 - f/4);
    float3 n;
    n.xy = fenc*g;
    n.z = 1 - f/2;
    return n;
}
\end{lstlisting}

The \textbf{b}-component of \emph{RT1} contains the view space linear depth [0...far plane].

Storing normal vector and depth information within one render target is useful for the SSAO render effect. It just needs the texture from this render target.




\subsubsection{GBuffer RT2}
The \textbf{rgb}-components of \emph{RT2} contain the specular color. The specular color is calculated using \emph{SpecularMap.rgb * SpecularColor.rgb}. The \textbf{a}-component of \emph{RT2} contains the specular exponent and is calculated using \emph{SpecularMap.a * SpecularExponent}. As result, if a \emph{SpecularMap} has an alpha channel, it's used for per texel specular power control.




\subsubsection{GBuffer RT3}
The \textbf{rgb}-components of \emph{RT3} contains the composition of emissive maps and light maps. Alpha is for glow. (outshine effect) 
Lighting, wich is not connected to a particular realtime light, is also rendered during the GBuffer fill. 




\subsection{Lighting}


\paragraph{BRDF Model}$\;$ \\
As BRDF model, Blinn-Phong with half vector specular highlights was chosen.


\paragraph{Gamma correction}$\;$ \\
Usually, color textures like hand-painted images or photos are stored in sRGB space, therefore, they must be converted from sRGB to linear space during rendering. This is automatically done for the texture maps of projective light sources. If this wouldn't be done, the colors of this texture maps would look bleached out.
Cg source~code~\ref{Code::LightingGammaCorrection} shows how the used gamma correction technique.
\begin{lstlisting}[float=htb,label=Code::LightingGammaCorrection,caption={GBuffer gamma correction}]
float3 linearRGBColor = pow(sRGBColor, 2.2);
\end{lstlisting}




\section{Post process effects}
PixelLight comes with a compact and comfortable post processing (is also called 'image processing') system. For image processing like bloom the scene is rendered into a texture instead the usual output. Then the post processing manager is taking this texture and applies different effects on it. At the end you will receive the final composition you can draw on screen using for instance a full screen rectangle. In today's hardware, high dynamic range (HDR) rendering must also be  implemented as image processing because of the lack of the output devices like monitor to display such HDR data directly. Therefore this HDR data must be mapped to the 'usual' RGB data using tone mapping.

The post processing system of PixelLight can be subdivided into two categories:
\begin{itemize}
\item The Scene Render Pipeline Post Processing integrates into the scene render
process and is using for example data from the deferred rendering GBuffer. This type of post processing is fixed build in.
\item Post Post Processing is completely decoupled from the scene render process
and is performed after the scene rendering is finished. This type of post processing is not fixed build in.
\end{itemize}




\subsection{Scene Render Pipeline Post Processing}
\paragraph{Depth Fog}$\;$ \\
Classic depth fog is realized as a post processing effect. Three fog modes are implemented:
\begin{itemize}
\item{LinearMode: Fog effect intensifies linearly between the start and end points $(f=(end-d)/(end-start))$}
\item{ExponentialMode: Fog effect intensifies exponentially $(f=1/((e^(d*density))))$}
\item{Exponential2Mode: Fog effect intensifies exponentially with the square of the distance $(f=1/((e^((d*density)^2))))$}
\end{itemize}
This effect is implemented within the \emph{PLCompositing::SRPDeferredDepthFog} scene renderer pass.


\paragraph{EdgeAA}$\;$ \\
Anti-aliasing is realized using resolution-independent edge detection as described within \url{http://http.developer.nvidia.com/GPUGems3/gpugems3_ch19.html}. This effect is implemented within the \emph{PLCompositing::SRPDeferredEdgeAA} scene renderer pass.


\paragraph{Screen-Space Ambient Occlusion (SSAO)}$\;$ \\
SSAO is calculated dynamically during runtime using per fragment depth and optionally also normal data. SSAO overwrites the static ambient occlusion value from GBuffer RT0. As a result, you can either have static ambient occlusion maps, or dynamic ambient occlusion, but not both at the same time. SSAO can be implemented in several ways. Currently the following techniques are implemented:
\begin{itemize}
\item{PLCompositing::SRPDeferredHBAO: Horizon Based Ambient Occlusion (HBAO) as described within the NVIDIA Direct3D SDK 10 Code Samples \url{http://developer.download.nvidia.com/SDK/10.5/direct3d/Source/ScreenSpaceAO/doc/ScreenSpaceAO.pdf}}
\item{PLCompositing::SRPDeferredHDAO: High Definition Ambient Occlusion (HDAO) as described within the ATI Radeon SDK \url{http://developer.amd.com/Downloads/HDAO10.1.zip}}
\end{itemize}
The concrete SSAO implementations are derived from the \emph{PLCompositing::SRPDeferredSSAO} class which offers \emph{Cross Bilateral Filter} as described within the NVIDIA Direct3D SDK 10 Code Samples \url{http://developer.download.nvidia.com/SDK/10.5/direct3d/Source/ScreenSpaceAO/doc/ScreenSpaceAO.pdf}.


\paragraph{Volumetric Light Scattering}$\;$ \\
\emph{Volumetric Light Scattering} as described within \url{http://http.developer.nvidia.com/GPUGems3/gpugems3_ch13.html}. This effect is also known as crepuscular rays, sunbeams, sunbursts, star flare, god rays, sun shafts, or light shafts. Within PixelLight, we chosen the name \emph{god rays} because it's short. The emissive/light map content of the GBuffer is used as glowing parts. This effect is implemented within the \emph{PLCompositing::SRPDeferredGodRays} scene renderer pass.


\paragraph{Glow}$\;$ \\
This effect is implemented within the \emph{PLCompositing::SRPDeferredGlow} scene renderer pass and is loosely basing on the technique described within \url{http://http.developer.nvidia.com/GPUGems/gpugems_ch21.html}.


\paragraph{Depth Of Field}$\;$ \\
This effect is implemented within the \emph{PLCompositing::SRPDeferredDOF} scene renderer pass and is using the technique described within \url{http://ati.amd.com/developer/gdc/Scheuermann_DepthOfField.pdf}.


\paragraph{HDR Tone Mapping}$\;$ \\
The scene render pass \emph{SRPEndHDR} finishes the HDR render pipeline by converting the HDR image using tone mapping into an LDR image (the available color range is compressed). As tone mapping operator we've chosen \emph{Reinhard tone mapping} as described within \url{http://www.cs.ucf.edu/~reinhard/cdrom/}.

For the tone mapping, the logarithmic average luminance of the current HDR image is required. While calculating this luminance value on the CPU is trivial, a parallel approach is required for calculating this value on the GPU. Within the literature, there are many ways this logarithmic average luminance can be calculated on the GPU. We decided to use the technique described within \url{http://developer.download.nvidia.com/SDK/9.5/Samples/DEMOS/Direct3D9/HDR_FP16x2.zip}, it looks like this is one of the more popular ways to solve the problem. The technique consists of three steps:
\begin{itemize}
\item{First downsample pass with calculation of the pixel luminance and log calculation}
\item{Downsample the 1 component texture until it has a size of 4x4 pixels}
\item{Calculate the final 1x1 and it's exponential value}
\end{itemize}
Although the last step is a waste of the tremendous GPU power, it's more efficient than downloading the result to the CPU and passing on the logarithmic average luminance to the tone mapping fragment shader.

For \emph{light adaptation}, the Pattanaik exponential decay function described within \url{http://www.coretechniques.info/PostFX.zip} is used. By using this technique, the change of the logarithmic average luminance is smoothed to simulate the gradual adaptation of the human eye to different lighting conditions.

\emph{HDR bloom} is also supported.
% [TODO] Any references? (bloom is not that complex :)
% http://www.gamasutra.com/features/20061003/kylmamaa_03.shtml
% http://harkal.sylphis3d.com/2006/05/20/how-to-do-good-bloom-for-hdr-rendering/

This render pass also performs \emph{gamma correction} as described within \url{http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html} and \url{http://www.weltenbauer.com/upload/dateien/gamma_correct_v12.pdf}.
Cg source~code~\ref{Code::PostProcessingGammaCorrection} shows how the used gamma correction technique.
\begin{lstlisting}[float=htb,label=Code::PostProcessingGammaCorrection,caption={Post processing gamma correction}]
// By default, gamma is 2.2
float3 sRGBColor = pow(linearRGBColor, 1/gamma);
\end{lstlisting}




\subsection{Post Post Processing}
The post processing that is performed after the scene rendering is finished, is completely based on script like XML files defining how a post process is created. Multiple post processes can be performed one after another.

\begin{itemize}
\item Each post process material can have annotation for additional information. (Material parameters fScaleX \& fScaleY, default value for both 1.0f)
\item Each post process material can have different kernels which are converted from pixel space to texel space automatically.\\
      TexelKernel[n].x = PixelKernel[n].x/width\\
      TexelKernel[n].y = PixelKernel[n].y/height\\
      were width and height are the dimension of the render target.
\item Material Parameter 'TargetDimension' will receive the render target width and height
\item Some special post process effects have special parameters - in this case you have to derive an own class from 'PostProcess' and implement the update of this special material parameters by self.
\item 2D and rectangle texture targets are supported
\end{itemize}

Here's a list of some provided post processing effects defined within PLPostProcessEffects.zip:\\
('->' shows the order in which post-process materials are used)\\
('*>' if a special post process class is used to for instance animate material parameters)\\
\begin{itemize}
\item Inverse: Inverses the colors (negative image)\\
      -> ColorInverse.mat
\item Monochrome: Grayscale image\\
      -> ColorMonochrome.mat
\item Sepia: Manipulates the colors\\
      -> ColorSepia.mat
\item Blur: Blurs the image\\
      -> ColorDownFilter4.mat + ColorGBlurH.mat + ColorGBlurV.mat + ColorUpFilter4.mat
\item Bloom: Bright things 'glow'\\
      -> ColorDownFilter8.mat + ColorBrightPass.mat + ColorBloomH.mat + ColorBloomV.mat + ColorBloomH.mat +	ColorBloomV.mat + ColorCombine8.mat
\item HDR (tone map): Nice high-dynamic-range rendering. A floating point render target is used instead the standard RGB unsigned char format. As post process a tone map effect is applied.\\
      -> ColorToneMap.mat
\item Edge detect: Edge detection\\
      -> ColorEdgeDetect.mat
\item Edge glow: Edges will glow\\
      -> ColorEdgeDetect.mat + ColorDownFilter4.mat + ColorBloomH.mat + ColorBloomV.mat + ColorCombine4.mat
\item Old film: The image looks like it was filmed with a very old camera. Image errors appear, the colors are a bit 'instable' and the image is wobble.\\
      -> ColorOldFilm.mat\\
      *> PostProcessOldFilm
\item Sketch: Looks like a pencil drawing\\
      -> ColorEdgeDetect.mat + ColorInverse.mat\\
        (+ ColorOldFilm.mat if it should look like a sketch of a cartoon :)
\item Cartoon: Looks like a cartoon because there are black silhouettes\\
      -> ColorEdgeDetect.mat + ColorInverse.mat + (ColorOldFilm.mat for animated edges) + ColorCombineMul.mat\\
        (+ ColorOldFilm.mat if it should look like an old cartoon :)
\item ASCII: Image is visualized using ASCII characters\\
      -> ColorDownFilter16.mat + PostProcess/ColorASCIIUp16.mat
\item Pull: The image is deformed at a given position\\
      -> ColorPull.mat
\item Pixel: The image has a low resolution so you can see the single pixels\\
      -> ColorDown4.mat + ColorUp4.mat
\end{itemize}

... even more combinations are possible - you can also tweak the parameters of the effect material. You can use the demo application \emph{PLDemoPostProcess.exe} to see this effects in action or to test your own or new effects.




\subsubsection{Post process file format}
Here's a short post process file ('pp' extension) example:

\begin{lstlisting}[caption=Post process file example]
<?xml version="1.0"?>
<PostProcess Version="1">
  <General TextureFormat="R8G8B8" />
  <Pass Material="PostProcess/ColorEdgeDetect.mat" />
  <Pass Material="PostProcess/ColorInverse.mat" />
  <Pass Class="PostProcessOldFilm" Material="PostProcess/ColorOldFilm.mat" />
</PostProcess>
\end{lstlisting}

And here's the DTD of this format:

\begin{lstlisting}[caption=Post process file format DTD]
<?xml version="1.0"?>
<!DOCTYPE PostProcess [
  <!ELEMENT General EMPTY>
  <!ATTLIST General TextureFormat CDATA #IMPLIED>
  <!ELEMENT Pass EMPTY>
  <!ATTLIST Pass Material CDATA #REQUIRED>
]>
\end{lstlisting}

As you can see, within a post process file, there are optional general information defining for instance the required render to texture format. Default setting for 'TextureFormat' is 'R8G8B8'. 'TextureFormat' can also be R8G8B8A8 if an alpha channel is required or R16G16B16A16F/R32G32B32A32F for floating point formats. The different passes are the main thing of this format. Each pass is in fact a material which will be applied to the current render to texture result. By using different passes after each other one can create different final effects. 'Pass' element can have a special class controlling/animating for instance certain unique shader parameters. The main work of the post processes is done within the materials and shaders.
